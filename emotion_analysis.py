# -*- coding: utf-8 -*-
"""
Automatically generated by Colaboratory.

# 事前設定
"""

import codecs
import json
import requests

"""## 利用情報を設定"""

ACCESS_TOKEN_PUBLISH_URL = "***"
DEVELOPER_API_BASE_URL = "***"

# 以下は利用者ごとに変える
CLIENT_ID = "JWz3ehwjqH9U4aBH2C0UWNICxUvdSstf"
CLIENT_SECRET = "ssQtM4v3XzKrX73W"

"""## API利用に必要なアクセストークンを取得"""

def getAccessToken(client_id, client_secret):
    headers = {
        "Content-Type": "application/json"
    }
    data = {
        "grantType": "client_credentials",
        "clientId": client_id,
        "clientSecret": client_secret
    }
    r_post = requests.post(ACCESS_TOKEN_PUBLISH_URL, headers=headers, json=data)
    r_json = r_post.json()
    return r_json["access_token"]

access_token = getAccessToken(CLIENT_ID, CLIENT_SECRET)

print (access_token)

"""↑24時間使えるアクセストークン

## 独自関数を定義
"""

# JSONを見やすくするためのprint関数を定義
def print_json(data):
    print (codecs.decode(json.dumps(data, indent=4, separators=(',', ': ')), 'unicode-escape'))

"""# 4.COTOHA API"""

# 感情分析APIを実行
def sentiment(sentence):
    url = DEVELOPER_API_BASE_URL + "v1/sentiment"
    headers = {
        "Content-Type": "application/json",
        "Authorization": "Bearer "+ access_token
    }
    data = {
        "sentence": sentence
    }
    r_post = requests.post(url, headers=headers, json=data)
    r_json = r_post.json()
    return r_json

  
# 入力文
document = "今日は成人の撮影でも作ってないよーって言っていきたい焼きどこからです。イチコイ最終回！。平成最後の撮影してくれました。ビジュアルコメンタリーを選択するファンクラブにおいては会報誌の積極的な"
  
# 感情分析APIを実行
sentiment_json = sentiment(document)

# 感情分析結果を表示
print_json(sentiment_json)

#感情分析必要な結果だけ欲しい時
#print_json(sentiment_json["result"])
#print_json(sentiment_json["result"]["sentiment"])　      #posi nega newtral 感情
#print_json(sentiment_json["result"]["score"])               #0-1 ポジティブ度。数が大きいとすごいポジ
#print_json(sentiment_json["result"]["emotional_phrase"]) # 文の中からポジティブな単語を抽出
#print_json(sentiment_json["result"]["emotional_phrase"][0])
#print_json(sentiment_json["result"]["emotional_phrase"][0]["form"])
#print_json(sentiment_json["result"]["emotional_phrase"][0]["emotion"])

"""# 5.名詞を抽出"""

# 構文解析APIを実行
def parse(sentence):
    url = DEVELOPER_API_BASE_URL + "v1/parse"
    headers = {
        "Content-Type": "application/json",
        "Authorization": "Bearer "+ access_token
    }
    data = {
        "sentence": sentence       
    }
    r_post = requests.post(url, headers=headers, json=data)
    r_json = r_post.json()
    return r_json

  
# 入力文
sentence = "私は美味しいご飯を食べた。"
  
# 構文解析APIを実行
parse_json = parse(sentence)

# 構文解析結果を表示
print_json(parse_json)  
  
  
# 文節ごとに分割
for chunk in parse_json["result"]:
    # 形態素ごとに分割
    #print(chunk)
    for token in chunk["tokens"]:
        #print(token)
        #print(token["form"])
        #if (token["kana"] == ""):  #。の抽出
        #    print(token["form"])
        if (token["pos"] == "名詞"):  #名詞のみの抽出
            print(token["form"])
            print("----")
            next



"""# 6.主語と述語を抽出"""

# 構文解析APIを実行
def parse(sentence):
    url = DEVELOPER_API_BASE_URL + "v1/parse"
    headers = {
        "Content-Type": "application/json",
        "Authorization": "Bearer "+ access_token
    }
    data = {
        "sentence": sentence       
    }
    r_post = requests.post(url, headers=headers, json=data)
    r_json = r_post.json()
    return r_json

  
# 入力文
sentence = "私は美味しいご飯を食べた。"
  
# 構文解析APIを実行
parse_json = parse(sentence)

# 構文解析結果を表示
#print_json(parse_json)  

  
# 文節ごとに分割
for chunk in parse_json["result"]:
    print_json(chunk["chunk_info"])
    for link in chunk["chunk_info"]["links"]:
        if link["label"] == "agent":
            # 主語のtokensを表示
            #print (parse_json["result"][link["link"]]["tokens"])
            for token in parse_json["result"][link["link"]]["tokens"]:
                print(token["form"])
                           
            # 述語のtokensを表示
            #print (chunk["tokens"])

"""#7.他のAPI

## 固有表現抽出API

### 固有表現抽出APIは、入力として日本語で記述された文を受け取り、人名や地名、日付表現(時間、日付)、組織名、量的表現(金額、割合)、人工物の8種類の固有表現と、200種類以上のクラス数を持つ拡張固有表現を出力します。
"""

# 固有表現抽出APIを実行
def ne(sentence):
    url = DEVELOPER_API_BASE_URL + "v1/ne"
    headers = {
        "Content-Type": "application/json", 
        "Authorization": "Bearer "+ access_token
    }
    data = {
        "sentence": sentence
    }
    r_post = requests.post(url, headers=headers, json=data)
    r_json = r_post.json()
    return r_json

  
# 入力文
sentence = "田中さんは埼玉県でアメリカの美味しいご飯を食べた。"
  
# 固有表現抽出APIを実行
ne_json = ne(sentence)

# 固有表現抽出結果を表示
print_json(ne_json)

"""## 照応解析API

### 照応解析APIは、入力として日本語で記述された複数の文からなるテキストを受け取り、テキスト中の「そこ」「それ」などの指示詞や「彼」「彼女」などの代名詞と対応する先行詞を抽出し、同一のものとしてまとめて出力します。
"""

# 照応解析APIを実行
def coreference(document):
    url = DEVELOPER_API_BASE_URL + "v1/coreference"
    headers = {
        "Content-Type": "application/json",
        "Authorization": "Bearer "+ access_token
    }
    data = {
        "document": document
    }
    r_post = requests.post(url, headers=headers, json=data)
    r_json = r_post.json()
    return r_json


# 入力文
document = "私は美味しいご飯を食べた。それはとても貴重だった。"
  
# 照応解析APIを実行
coreference_json = coreference(document)

# 照応解析結果を表示
print_json(coreference_json)

"""## キーワード抽出API

### キーワード抽出APIは、入力として日本語で記述された複数の文からなるテキストを受け取り、テキストに含まれる特徴的なフレーズ・単語をキーワードとして抽出します。テキストから算出される特徴的スコアに基づいて、複数のフレーズ・単語が降順に出力されます。
"""

# キーワード抽出APIを実行
def keyword(document):
    url = DEVELOPER_API_BASE_URL + "v1/keyword"
    headers = {
        "Content-Type": "application/json",
        "Authorization": "Bearer "+ access_token
    }
    data = {
        "document": document
    }
    r_post = requests.post(url, headers=headers, json=data)
    r_json = r_post.json()
    return r_json

  
# 入力文
document = "ここに文字列を代入"

# キーワード抽出APIを実行
keyword_json = keyword(document)

# キーワード抽出結果を表示
print_json(keyword_json)

"""## 類似度算出API

### 類似度算出APIは、入力として日本語で記述されたテキストを2つ受け取り、テキスト間の意味的な類似度を算出・出力します。類似度は0から1の値域で出力され、1に近づくほどテキスト間の類似性が大きいことを示します。テキストに含まれる単語の意味情報を用いて類似度を算出しているため、異なった単語を含むテキスト間の類似性も推定することができます。
"""

# 類似度算出APIを実行
def similarity(sentence1, sentence2):
    url = DEVELOPER_API_BASE_URL + "v1/similarity"
    headers = {
        "Content-Type": "application/json",
        "Authorization": "Bearer "+ access_token
    }
    data = {
        "s1": sentence1,
        "s2": sentence2
    }
    r_post = requests.post(url, headers=headers, json=data)
    r_json = r_post.json()
    return r_json

  
# 入力文
sentence1 = "私は美味しいご飯を食べた。"
sentence2 = "今日は気分が悪い。"
  
# 類似度算出APIを実行
similarity_json = similarity(sentence1, sentence2)

# 類似度算出結果を表示
print_json(similarity_json)

"""## 文タイプ判定API

### 文タイプ判定APIは、入力として日本語で記述された文を受け取り、文の法(叙述/疑問/命令)タイプと発話行為タイプを判定・出力します。
"""

# 文タイプ判定APIを実行
def sentence_type(sentence):
    url = DEVELOPER_API_BASE_URL + "v1/sentence_type"
    headers = {
        "Content-Type": "application/json",
        "Authorization": "Bearer "+ access_token
    }
    data = {
        "sentence": sentence
    }
    r_post = requests.post(url, headers=headers, json=data)
    r_json = r_post.json()
    return r_json

  
# 入力文
document = "ここに文字を代入"　#1疑問文なのか　情報提供を求めている

# 文タイプ判定APIを実行
st_json = sentence_type(document)

# 文タイプ判定結果を表示
print_json(st_json)

"""## ユーザ属性推定API

### ユーザ属性推定APIは、入力として日本語で記述された複数の文からなるテキストを受け取り、年代、性別、趣味、職業などの人物に関する属性を推定・出力します。
### ※本APIはベータ版での提供となっております。
"""

# ユーザ属性推定APIを実行
def user_attribute(document):
    url = DEVELOPER_API_BASE_URL + "beta/user_attribute"
    headers = {
        "Content-Type": "application/json",
        "Authorization": "Bearer "+ access_token
    }
    data = {
        "document": document
    }
    r_post = requests.post(url, headers=headers, json=data)
    r_json = r_post.json()
    return r_json

  
# 入力文
document = "今日は成人の撮影でも作ってないよーって言っていきたい焼きどこからです。イチコイ最終回！。平成最後の撮影してくれました。ビジュアルコメンタリーを選択するファンクラブにおいては会報誌の積極的な"
  
# ユーザ属性推定APIを実行
ua_json = user_attribute(document)

# ユーザ属性推定結果を表示
print_json(ua_json)

"""## 言い淀み除去API

### 言い淀み除去APIは、音声認識処理後のテキストに対して、ユーザからの音声入力時に含まれる言い淀みを除去します。
### ※本APIはベータ版での提供となっております。
"""

# 言い淀み除去APIを実行
def remove_filler(text):
    url = DEVELOPER_API_BASE_URL + "beta/remove_filler"
    headers = {
        "Content-Type": "application/json",
        "Authorization": "Bearer "+ access_token
    }
    data = {
        "text": text,
        "do_segment": True
    }
    r_post = requests.post(url, headers=headers, json=data)
    r_json = r_post.json()
    return r_json

  
# 入力文
text = "えーっと私はあの美味しいご飯を食べた。"

# 言い淀み除去APIを実行
rf_json = remove_filler(text)

# 言い淀み除去結果を表示
print_json(rf_json)

"""## 音声認識結果誤り検知API

### 音声認識結果誤り検知APIは、音声認識処理後のテキストを受け取り、認識誤りが疑われる箇所をそのスコアとともに出力します。入力文全体の誤り度合いについても数値化を行って出力します。
### ※本APIはベータ版での提供となっております。
"""

# 音声認識結果誤り検知APIを実行
def detect_misrecognition(sentence):
    url = DEVELOPER_API_BASE_URL + "beta/detect_misrecognition"
    headers = {
        "Content-Type": "application/json",
        "Authorization": "Bearer "+ access_token,
    }
    data = {
        "sentence": sentence
    }
    r_post = requests.post(url, headers=headers, json=data)
    r_json = r_post.json()
    return r_json

  
# 入力文
sentence = "証は美味しいご飯を食べた"

# 音声認識結果誤り検知APIを実行
dm_json = detect_misrecognition(sentence)

# 音声認識結果誤り検知結果を表示
print_json(dm_json)

"""# 8.歌詞を要約してみよう

## 解析情報設定
"""

# 入力となる歌詞を設定
lyrics = ""

"""## 歌詞からキーワードを抽出する"""

MAX_KEYWORD_NUM = 5

def keyword(document):
    url = DEVELOPER_API_BASE_URL + "v1/keyword"
    headers = {
        "Content-Type": "application/json",
        "Authorization": "Bearer "+ access_token
    }
    data = {
        "document": document,
        "max_keyword_num": MAX_KEYWORD_NUM
    }
    r_post = requests.post(url, headers=headers, json=data)
    r_json = r_post.json()
    return r_json


keyword_json = keyword(lyrics)


# 各キーワードを抽出
keywords = []
for key_word in keyword_json["result"]:
    # キーワードを表示
    keywords.append(key_word["form"])
    print (key_word["form"])

"""## キーワードを含む文節（キーワード文節）を取得する"""

def parse(sentence):
    url = DEVELOPER_API_BASE_URL + "v1/parse"
    headers = {
        "Content-Type": "application/json",
        "Authorization": "Bearer "+ access_token,
    }
    data = {
        "sentence": sentence
    }
    r_post = requests.post(url, headers=headers, json=data)
    r_json = r_post.json()
    return r_json
  

parse_json = parse(lyrics)

            
# キーワードが出てくる文節(キーワード文節)抽出
keyword_chunks = []
for chunk in parse_json["result"]:
    for word in chunk["tokens"]:
        if(word["form"] in keywords):
            for token in chunk["tokens"]:
                print (token["form"], end='')
            keyword_chunks.append(chunk["chunk_info"]["id"])
            print ('')
print (keyword_chunks)

"""## キーワード文節の係り元文節（ベース文節）を取得する"""

keyword_base_chunks = []
# キーワード文節に係る文節(ベース文節)取得
for chunk in parse_json["result"]:
    for link_chunks in chunk["chunk_info"]["links"]:
        if (link_chunks["link"] in keyword_chunks):
            for token in chunk["tokens"]:
                print (token["form"], end='')
            keyword_base_chunks.append(chunk["chunk_info"]["id"])
            print ('')
print (keyword_base_chunks)

"""## ベース文節ごとに、係り先文節一覧を取得する"""

keyword_tree = {}
# ベース文節から係る文節取得
for chunk in parse_json["result"]:
    if (chunk["chunk_info"]["id"] in keyword_base_chunks):
        keyword_tree[chunk["chunk_info"]["id"]] = [chunk["chunk_info"]["id"]]
        for link_chunks in chunk["chunk_info"]["links"]:
            keyword_tree[chunk["chunk_info"]["id"]].append(link_chunks["link"])
print (keyword_tree)

"""## ベース文節ごとの係り先文節を結合し、要約節とする"""

# ベース文節ごとにまとめて表示
lyrics_summary = []
for key_word in keyword_tree:
    lyrics_summary.append('')
    for chunk in parse_json["result"]:
        if chunk["chunk_info"]["id"] in keyword_tree[key_word]:
            for token in chunk["tokens"]:
                lyrics_summary[-1] += token["form"]

print (lyrics_summary)

"""## 類似している要約詞は削除する"""

def similarity(sentence1, sentence2):
    url = DEVELOPER_API_BASE_URL + "v1/similarity"
    headers = {
        "Content-Type": "application/json",
        "Authorization": "Bearer "+ access_token
    }
    data = {
        "s1": sentence1,
        "s2": sentence2
    }
    r_post = requests.post(url, headers=headers, json=data)
    r_json = r_post.json()
    return r_json

  
# 似ている文は後に出現したもののみ採用
SIMILAR_SCORE = 0.9
dup_list = []
for i in range(0, len(lyrics_summary)):
    # 入力文と過去分の類似度を比較
    similar = False
    for j in range(i+1, len(lyrics_summary)):
        similarity_json = similarity(lyrics_summary[i], lyrics_summary[j])
        similarity_score = similarity_json["result"]["score"]
        print (lyrics_summary[i], lyrics_summary[j], similarity_score)
        if similarity_score > SIMILAR_SCORE:
            dup_list.append(i)
            break

print (dup_list)
for i, dup in enumerate(dup_list):
    print (dup, i)
    del lyrics_summary[dup-i]

print ("要約歌詞")
for ly_sum in lyrics_summary:
    print (ly_sum)

"""## キーワードだけでなく、感情語も利用する"""

def sentiment(sentence):
    url = DEVELOPER_API_BASE_URL + "v1/sentiment"
    headers = {
        "Content-Type": "application/json",
        "Authorization": "Bearer "+ access_token
    }
    data = {
        "sentence": sentence,
        "do_segment": True
    }
    r_post = requests.post(url, headers=headers, json=data)
    r_json = r_post.json()
    return r_json
  

# ポジティブネガティブ表現変換
pn_converter = {"Positive":"P", "Negative":"N", "Neutral":"PN"}
# 感情語一覧(form)
emotional_phrases = []


sentiment_json = sentiment(lyrics)
sentiment_json


# 歌詞全体の感情を取得
sent = sentiment_json["result"]["sentiment"]
sent = pn_converter[sent]
# 各感情語を抽出
for emotional_phrase in sentiment_json["result"]["emotional_phrase"]:
    # 歌詞感情と合致するキーワードを表示
    if emotional_phrase["emotion"] == sent:
        emotional_phrases.append(emotional_phrase["form"])
        print (emotional_phrase["form"])
    next
    
keywords += emotional_phrases

"""## 句読点を打つ"""

def remove_filler(text):
    url = DEVELOPER_API_BASE_URL + "beta/remove_filler"
    headers = {
        "Content-Type": "application/json",
        "Authorization": "Bearer "+ access_token
    }
    data = {
        "text": text,
        "do_segment": True
    }
    r_post = requests.post(url, headers=headers, json=data)
    r_json = r_post.json()
    return r_json
  
  
rf_json = remove_filler(lyrics)
print (rf_json)

lyrics_document = ""
lyrics_sentence = []
for sentence in rf_json["result"]:
    lyrics_document += sentence["normalized_sentence"]
    lyrics_sentence.append(sentence["normalized_sentence"])

    
print (lyrics_document)
print (lyrics_sentence)
lyrics = lyrics_document